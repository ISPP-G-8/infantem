# Pilot Users Performance Evaluation

![Portada](../images/Infantem.png)
---
**Fecha:** 15/05/2025  
**Grupo 8:** Infantem  
**World Project Launch**

## Integrantes del Grupo
<div style="display: flex; justify-content: space-between; gap: 2px;">
  <div>
    <ul style="padding-left: 0; list-style: none;">
      <li>Álvaro Jiménez Osuna</li>
      <li>Ángela López Oliva</li>
      <li>Antonio Jiménez Ortega</li>
      <li>Daniel del Castillo Piñero</li>
      <li>David Fuentelsaz Rodríguez</li>
      <li>David Vargas Muñiz</li>
      <li>Enrique García Abadía</li>
      <li>Felipe Solís Agudo</li>
      <li>Javier Santos Martín</li>
    </ul>
  </div>

  <div>
    <ul style="padding-left: 0; list-style: none;">
    <li>Javier Ulecia García</li>
      <li>José García de Tejada Delgado</li>
      <li>Jose Maria Morgado Prudencio</li>
      <li>Josué Rodríguez López</li>
      <li>Lucía Noya Cano</li>
      <li>Luis Giraldo Santiago</li>
      <li>Miguel Galán Lerate</li>
      <li>Paula Luna Navarro</li>
    </ul>
  </div>
</div>

## Colaboradores del documento
- **Paula Luna Navarro:**
Creó el documento y escribió su contenido
---

Este documento presenta la **evaluación del desempeño de los usuarios piloto** a lo largo del desarrollo del proyecto *Infantem*, basada en los comentarios y valoraciones recogidas en los distintos entregables (Sprint 1, Sprint 2, Sprint 3 y PPL).  

Durante el proceso, se ha llevado a cabo una estrategia de validación iterativa mediante encuestas de feedback tras cada entrega funcional. Estas encuestas han permitido detectar errores, mejorar la experiencia de usuario y validar nuevas funcionalidades implementadas.

Gracias a esta dinámica, hemos podido **ajustar progresivamente el diseño, la navegación y el valor funcional de la app**, basándonos en **aportes reales de usuarios**, lo que ha fortalecido la toma de decisiones de cara al lanzamiento final.

Los resultados recogidos a continuación reflejan el grado de colaboración y la calidad del feedback aportado por cada participante, permitiendo así valorar su impacto dentro del proceso de mejora continua.



## Sistema de Evaluación

El siguiente sistema ha sido la **mecánica de evaluación utilizada de forma consistente durante todos los sprints y entregas del proyecto**. Ha servido como guía objetiva para asignar las puntuaciones a cada usuario piloto en base a la calidad, profundidad y utilidad de los comentarios proporcionados.

| **Puntuación** | **Descripción** |
|----------------|-----------------|
| 10             | Proporciona comentarios detallados y sugerencias que van más allá de lo solicitado, evidenciando una exploración profunda del sistema. |
| 8-9            | Completa la evaluación con información útil, aportando observaciones que ayudan a mejorar el producto. |
| 6-7            | Entrega el formulario completo, aunque con respuestas algo superficiales o sin comentarios extra. |
| 5              | Presenta el documento de forma básica, sin mostrar mucho interés en el proceso. |
| 1-4            | Rellena el documento de forma incompleta o sin haber probado correctamente la aplicación. |
| 0              | No entrega el documento de evaluación. |

---

A continuación se muestra un **breve resumen individualizado** de cada usuario piloto, detallando el trabajo realizado, los comentarios aportados y su contribución específica a la evolución del producto.

###  **Resumen global: Natalia Olmo**

Natalia ha demostrado una participación constante y comprometida durante todos los sprints. Desde el principio aportó observaciones valiosas sobre la lógica de uso, la experiencia en dispositivos móviles y la validación de datos. Su feedback fue especialmente útil en la identificación de errores en formularios, usabilidad de filtros, y claridad en la presentación de contenidos. A lo largo del proyecto, evolucionó hacia una visión más crítica y constructiva, destacando fallos específicos en campos técnicos como los filtros de edad o las validaciones visuales. Su enfoque fue siempre práctico, destacando por su sensibilidad a la experiencia real del usuario y la claridad de la interfaz.

---

### **Resumen global: Antonio Macías**

Antonio ha sido uno de los usuarios más implicados y constantes, con una evolución clara desde comentarios más generales hacia un análisis profundo y técnico. Su foco principal ha estado en la mejora visual, la accesibilidad y la organización de contenidos, además de aportar ideas realistas sobre monetización, integración con Google Calendar y funcionalidades como guardar recetas favoritas. También propuso mejoras visuales en formularios, estructura de navegación y presentación del marketplace. A lo largo del proyecto se mostró atento a los detalles, crítico con las limitaciones técnicas y con una actitud muy proactiva hacia la mejora de la app.

---

### **Resumen global: Ramón Gavira**

Ramón ha sido el evaluador más crítico y técnico del grupo. Desde el Sprint 1 detectó problemas de fondo en la lógica de uso, la estructura de la app, y la falta de claridad en la navegación. A lo largo del proyecto fue escalando en profundidad, identificando errores técnicos como bugs en formularios, fallos de validación, errores 500 y problemas de visualización. Su enfoque siempre ha sido profesional, proponiendo soluciones robustas y estructurales, como reescritura de formularios, refactor visual y definición clara del sistema de alérgenos. Ha sido fundamental para detectar fallos que afectan directamente a la estabilidad y calidad del producto.


###  **Resumen global: Alberto Carmona**

Alberto ha mantenido una evaluación constante, detallada y centrada en la **usabilidad, precisión técnica y experiencia visual**. Desde los primeros sprints destacó fallos clave en la interfaz de usuario, validaciones y diseño visual. Su feedback fue especialmente útil para identificar errores silenciosos, falta de retroalimentación visual y problemas en la gestión de formularios y alérgenos. A lo largo del proyecto ha evolucionado hacia propuestas cada vez más completas, incluyendo mejoras funcionales, sugerencias técnicas viables (como el uso de almacenamiento externo para imágenes o vídeos) y organización del marketplace. Su participación ha sido altamente valiosa para perfeccionar la experiencia de usuario.

---

### **Resumen global: Santiago Rosado**

Santiago ha ofrecido un enfoque equilibrado entre la **usabilidad y la presentación visual**, con especial sensibilidad hacia la claridad de navegación, la estructura de los formularios y la percepción general del producto. Desde el principio aportó ideas sobre la presentación de información, sugerencias para gráficos, y mejoras visuales en recetas. Posteriormente, sus observaciones se centraron en **errores de validación, experiencia de login, y formularios confusos**, así como en propuestas de monetización y categorización del marketplace. Su evolución refleja un usuario que ha acompañado el crecimiento del proyecto con feedback progresivamente más técnico y estratégico.

---

### **Resumen global: Raúl Heras**

Raúl ha sido un usuario constante en sus evaluaciones, aportando un enfoque muy **centrado en la experiencia práctica de uso**, la claridad visual y la funcionalidad básica. Sus comentarios han sido siempre constructivos, señalando fallos de presentación, navegación y errores visuales que afectan a la interpretación de datos. A medida que avanzaban los sprints, sus aportaciones se enfocaron en la mejora del calendario, el registro de bebés y el formulario de recetas. También sugirió ideas de accesibilidad y mejoras visuales para hacer la app más intuitiva. Su participación ha sido clave para detectar problemas de usabilidad no evidentes y mejorar la navegación en flujos complejos.



---
# Evaluaciones Finales 
## Media de Evaluaciones por Usuario (Sprints 1–3 y PPL)

La nota final para el *Performance Evaluation* de los usuarios piloto ha sido calculada como la **media de todas las puntuaciones recogidas** en los distintos entregables del proyecto: Sprint 1, Sprint 2, Sprint 3 y la prueba piloto final (PPL). Esta media refleja de forma objetiva el nivel de implicación, calidad del feedback y utilidad de las aportaciones realizadas por cada participante durante todo el proceso de validación del producto.

| Usuario           | Sprint 1 (12/03/2025) | Sprint 2 (22/03/2025) | Sprint 3 (05/04/2025) | PPL (23/04/2025) |Nota Final WPL (18/05/2025) |
|------------------|-----------------------|------------------------|------------------------|------------------|------------------|
| Natalia Olmo     | 8                     | 9                      | 9                      | 9                | **8.75**         |
| Antonio Macías   | 9                     | 9                      | 9                      | 10               | **9.25**         |
| Ramón Gavira     | 10                    | 10                     | 10                     | 10               | **10.00**        |
| Alberto Carmona  | 10                    | 9                      | 10                     | 10               | **9.75**         |
| Santiago Rosado  | 8                     | 9                      | 9                      | 9                | **8.75**         |
| Raúl Heras       | 7                     | 8                      | 9                      | 8                | **8.00**         |

### Resumen de Calificaciones Finales (WPL)

Las evaluaciones finales reflejan un **alto nivel de implicación y compromiso por parte de los usuarios piloto** durante todas las fases del proyecto. La media más baja es de **8.00**, lo que indica que incluso los participantes con menor puntuación ofrecieron valoraciones consistentes y útiles para la mejora del producto.

- **Dos usuarios (Ramón Gavira y Alberto Carmona)** destacan con puntuaciones cercanas o iguales a la excelencia **10** y **9.75**, por sus aportaciones técnicas y detalladas, tanto en usabilidad como en funcionalidad.
- **La mayoría de usuarios (Natalia, Antonio, Santiago)** han mantenido una línea constante de colaboración con medias entre **8.75 y 9.25**, proporcionando feedback constructivo y bien argumentado.
- **El caso de Raúl Heras** también es positivo, con una media de **8.00**, siendo especialmente relevante su visión sobre la claridad y utilidad de la app, aunque con menos detalle técnico.

En conjunto, estos resultados avalan la calidad del proceso de validación piloto y demuestran que la app ha sido analizada en profundidad desde distintos perfiles de usuario, contribuyendo significativamente a su mejora continua.


---
> *Estas evaluaciones corresponden a los estudiantes que cursan la asignatura Ingeniería del Software para Proyectos Profesionales (ISPP), quienes han participado como usuarios piloto en la prueba de la aplicación. Sus valoraciones han sido registradas de manera individual para reflejar sus hallazgos y sugerencias de mejora. El resto de opiniones han sido agrupadas y resumidas en el apartado siguiente para ofrecer una visión más global del feedback recibido.*
